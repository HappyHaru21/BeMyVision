
# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rCK7B-Do65M-rRY_Cm6FS9WDgBFeZIVx
"""

!pip install transformers torch torchvision pillow opencv-python matplotlib gtts

from IPython.display import display, Javascript
from google.colab.output import eval_js

js_code = """
async function startVideoCapture() {
    // Create video element
    const video = document.createElement('video');
    video.style.width = '640px';
    video.style.height = '480px';
    document.body.appendChild(video);

    // Get webcam stream
    const stream = await navigator.mediaDevices.getUserMedia({ video: true });
    video.srcObject = stream;
    await video.play();

    // Show video feed in Colab
    google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);

    // Capture frames every 5 seconds
    while(true) {
        await new Promise(resolve => setTimeout(resolve, 5000)); // Wait for 5 seconds
        const canvas = document.createElement('canvas');
        canvas.width = video.videoWidth;
        canvas.height = video.videoHeight;
        canvas.getContext('2d').drawImage(video, 0, 0);

        let imgData = canvas.toDataURL('image/jpeg', 0.8);
        await google.colab.kernel.invokeFunction('notebook.process_frame', [imgData], {});
    }

}
startVideoCapture();
"""
display(Javascript(js_code))

import torch
from transformers import BlipProcessor, BlipForConditionalGeneration

# Load BLIP model
device = "cuda" if torch.cuda.is_available() else "cpu"
processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-large")
model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-large").to(device)

import base64
import io
from PIL import Image
from google.colab import output

# Convert base64 to PIL image
def decode_base64_image(data):
    header, encoded = data.split(",", 1)
    decoded = base64.b64decode(encoded)
    image = Image.open(io.BytesIO(decoded)).convert("RGB")
    return image

# Generate captions using BLIP
def generate_caption(image):
    inputs = processor(image, return_tensors="pt").to(device)
    generated_ids = model.generate(
        **inputs,
        max_new_tokens=80,
        min_length=10,
        do_sample=True,
        output_scores=True,
        return_dict_in_generate=True,
        temperature=0.2,
        top_p=0.6,
        num_beams=60,
        repetition_penalty=1.8,
        length_penalty=1,
        early_stopping=True,
    )

    # Access 'sequences' to get the token IDs
    generated_sequence = generated_ids.sequences[0]  # Select the first sequence
    caption = processor.decode(generated_sequence, skip_special_tokens=True)

    # Calculate confidence (if possible, otherwise you may need a different approach)
    log_probs = torch.stack(generated_ids.scores, dim=1).log_softmax(dim=-1)
    token_probs = torch.gather(log_probs, 2, generated_ids.sequences[:, 1:, None]).exp()
    avg_prob = token_probs.mean().item()

    return caption, avg_prob


# Process frames and print captions
def process_frame(img_data):
    try:
        image = decode_base64_image(img_data)
        caption,confidence = generate_caption(image)
        if confidence>0.5:
          print(f"üìù Caption: {caption} (Confidence:{confidence:.2f})")
        else:
          print("low confidence caption discarded")
    except Exception as e:
        print(f"Error processing frame: {e}")

# Register function for JavaScript to send images
output.register_callback('notebook.process_frame', process_frame)